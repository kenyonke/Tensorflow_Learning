import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.examples.tutorials.mnist import input_data

'''
#load minist image data
mnist = input_data.read_data_sets('.\mnist', one_hot=True)  # they has been normalized to range (0,1)
train_x = mnist.train.images[:4000]
train_y = mnist.train.labels[:4000]

test_x = mnist.test.images[4000:4300]
test_y = mnist.test.labels[4000:4300]



data_x = tf.placeholder(tf.float32,[None,784])
data_y = tf.placeholder(tf.int32,[None,10])
'''

'''
#softmax testing
w = tf.Variable(tf.zeros(shape=[784,10]), dtype=tf.float32, name='w')
b = tf.Variable(tf.zeros(shape=[10]),dtype=tf.float32,name='b')

pred = tf.nn.softmax(tf.add(tf.matmul(data_x,w),b))
loss = tf.losses.softmax_cross_entropy(data_y,pred)
train = tf.train.GradientDescentOptimizer(0.1).minimize(loss)
acc = tf.metrics.accuracy(labels=tf.argmax(data_y, axis=1), predictions=tf.argmax(pred, axis=1),)[1]

with tf.Session() as sess:
    # Initialize variables
    sess.run(tf.global_variables_initializer())
    sess.run(tf.local_variables_initializer())
    
    for i in range(500):
        loss_val,_ = sess.run([loss,train],feed_dict = {data_x:train_x,data_y:train_y})
        print(loss_val)
    
    accuracy = sess.run([acc],feed_dict = {data_x:test_x,data_y:test_y})
    print('accuracy:',accuracy)
'''

'''
#CNN testing
x = tf.reshape(data_x, [-1, 28, 28, 1])  

conv1 = tf.layers.conv2d(
        inputs = x,
        filters = 10,
        kernel_size = 5,
        strides = 1,
        padding = 'same',
        activation = tf.nn.rtelu
        ) 

pool1 = tf.layers.max_pooling2d(
        inputs = conv1,
        pool_size = 2,
        strides = 2
        )

conv2 = tf.layers.conv2d(pool1, 32, 5, 1, 'same', activation=tf.nn.relu)
pool2 = tf.layers.max_pooling2d(conv2,2,2)
cnn_output = tf.reshape(pool2, [-1, 7*7*32])

w = tf.Variable(tf.zeros(shape=[7*7*32,10]), dtype=tf.float32, name='w')
b = tf.Variable(tf.zeros(shape=[10]),dtype=tf.float32,name='b')

pred = tf.nn.softmax(tf.add(tf.matmul(cnn_output,w),b))
loss = tf.losses.softmax_cross_entropy(data_y,pred)
train = tf.train.AdamOptimizer(0.01).minimize(loss)
acc = tf.metrics.accuracy(labels=tf.argmax(data_y, axis=1), predictions=tf.argmax(pred, axis=1),)[1]

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    sess.run(tf.local_variables_initializer())   
    
    for i in range(200):
        #batch_x, batch_y = mnist.train.next_batch(BATCH_SIZE=200) 
        loss_val,_ = sess.run([loss,train],feed_dict = {data_x:train_x,data_y:train_y})
        print(loss_val)
    
    accuracy = sess.run([acc],feed_dict = {data_x:test_x,data_y:test_y})
    print('accuracy:',accuracy)    
'''

#RNN testing
feature_size = 1
label_size = 1
time_step = 10

x = tf.placeholder(tf.float32,[None, time_step, feature_size])
y = tf.placeholder(tf.float32,[None, time_step, label_size])

rnn_cell = tf.contrib.rnn.BasicRNNCell(num_units = 32)
init = rnn_cell.zero_state(batch_size=1, dtype=tf.float32)

# 'outputs' is a tensor of shape [batch_size, max_time, cell_state_size]
outputs,state = tf.nn.dynamic_rnn(
    rnn_cell,                # cell chosen
    x,                       # input
    #initial_state=None
    initial_state=init,  
    time_major=False, 
    )
outs2D = tf.reshape(outputs, [-1, 32]) 
net_outs2D = tf.layers.dense(outs2D, label_size)
outs = tf.reshape(net_outs2D, [-1, time_step, label_size]) 

loss = tf.losses.mean_squared_error(labels=y, predictions=outs)  # compute cost
train_op = tf.train.AdamOptimizer(0.01).minimize(loss)

sess = tf.Session()
sess.run(tf.global_variables_initializer())     # initialize var in graph

plt.figure(1, figsize=(12, 5))
plt.ion()       # continuously plot

for step in range(80):
    start = step * np.pi
    end = (step+1)*np.pi
    # use sin predicts cos
    steps = np.linspace(start, end, time_step)
    # x is the input but we need show time steps which is steps in ploting 
    # np.newaxis: insert new dimension
    # Also, b[np.newaxis] is equals to b[np.newaxis,:]
    train_x = np.sin(steps)[np.newaxis, :, np.newaxis]    # shape (batch, time_step, input_size)
    train_y = np.cos(steps)[np.newaxis, :, np.newaxis]
    
    if 'last_state' not in globals():                 # first state, no any hidden state
        feed_dict = {x: train_x, y: train_y}
    else:                                           # has hidden state, so pass it to rnn
        feed_dict = {x: train_x, y: train_y, init: last_state}
    _, pred_, last_state = sess.run([train_op, outs, state], feed_dict)     # train

    # plotting
    plt.plot(steps, train_y.flatten(), 'r-')
    plt.plot(steps, pred_.flatten(), 'b-')
    plt.ylim((-1.2, 1.2))
    plt.draw()

plt.ioff()
plt.show()
